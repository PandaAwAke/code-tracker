{
  "origin": "codeshovel",
  "repositoryName": "lucene-solr",
  "repositoryPath": "H:\\Projects\\apache\\lucene-solr/.git",
  "startCommitName": "38bf976cd4b9e324c21664bd7ae3d554df803705",
  "sourceFileName": "MemoryIndex.java",
  "functionName": "keywordTokenStream",
  "functionId": "keywordTokenStream___keywords-Collection__T__(modifiers-final)",
  "sourceFilePath": "lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
  "functionStartLine": 334,
  "functionEndLine": 361,
  "numCommitsSeen": 278,
  "timeTaken": 4466,
  "changeHistory": [
    "e8e4245d9b36123446546ff15967ac95429ea2b0",
    "ad0e49591148340418569d7c650761a6d41cf1b1",
    "778d96752fa94636a2136ea2b4d58a3fcbe283ec",
    "3f722b66a55ace117de6458f5b7d52f5bbc20c62",
    "786eb6ce0d19c6459f204b5d4ab0dc72245888cb",
    "786457c0e3ae6afa610788a9ffdfe6bf01c1f976",
    "ec90bc2202ef501e257eaf235be5ca15239c03c2",
    "1743081b078a3206e676bdd4ebe9203f5bad6c90",
    "bb6b7117186b656b4777850fdc463e0eaa541130",
    "e28541354d496a43078c1bc281076f97ed7d008c"
  ],
  "changeHistoryShort": {
    "e8e4245d9b36123446546ff15967ac95429ea2b0": "Yfilerename",
    "ad0e49591148340418569d7c650761a6d41cf1b1": "Ybodychange",
    "778d96752fa94636a2136ea2b4d58a3fcbe283ec": "Yfilerename",
    "3f722b66a55ace117de6458f5b7d52f5bbc20c62": "Ybodychange",
    "786eb6ce0d19c6459f204b5d4ab0dc72245888cb": "Ybodychange",
    "786457c0e3ae6afa610788a9ffdfe6bf01c1f976": "Ymultichange(Yparameterchange,Ybodychange)",
    "ec90bc2202ef501e257eaf235be5ca15239c03c2": "Ybodychange",
    "1743081b078a3206e676bdd4ebe9203f5bad6c90": "Ybodychange",
    "bb6b7117186b656b4777850fdc463e0eaa541130": "Ybodychange",
    "e28541354d496a43078c1bc281076f97ed7d008c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "e8e4245d9b36123446546ff15967ac95429ea2b0": {
      "type": "Yfilerename",
      "commitMessage": "LUCENE-3965: consolidate all api modules and fix packaging for 4.0\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1327094 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2012-04-17, 9:36 a.m.",
      "commitName": "e8e4245d9b36123446546ff15967ac95429ea2b0",
      "commitAuthor": "Robert Muir",
      "commitDateOld": "2012-04-17, 8:03 a.m.",
      "commitNameOld": "0daa4b0aac1748bbb2c56547626e9f49e7fb4ed6",
      "commitAuthorOld": "Sami Siren",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "actualSource": "  public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n    if (keywords \u003d\u003d null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n      private int start \u003d 0;\n      private final CharTermAttribute termAtt \u003d addAttribute(CharTermAttribute.class);\n      private final OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        T obj \u003d iter.next();\n        if (obj \u003d\u003d null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term \u003d obj.toString();\n        clearAttributes();\n        termAtt.setEmpty().append(term);\n        offsetAtt.setOffset(start, start+termAtt.length());\n        start +\u003d term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }",
      "path": "lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 282,
      "functionName": "keywordTokenStream",
      "diff": "",
      "extendedDetails": {
        "oldPath": "lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
        "newPath": "lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java"
      }
    },
    "ad0e49591148340418569d7c650761a6d41cf1b1": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-2372: switch over remaining uses of TermAttribute\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@950008 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2010-06-01, 6:35 a.m.",
      "commitName": "ad0e49591148340418569d7c650761a6d41cf1b1",
      "commitAuthor": "Robert Muir",
      "commitDateOld": "2016-01-22, 7:18 p.m.",
      "commitNameOld": "778d96752fa94636a2136ea2b4d58a3fcbe283ec",
      "commitAuthorOld": "Dawid Weiss",
      "daysBetweenCommits": -2061.57,
      "commitsBetweenForRepo": 0,
      "commitsBetweenForFile": 0,
      "actualSource": "  public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n    if (keywords \u003d\u003d null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n      private int start \u003d 0;\n      private final CharTermAttribute termAtt \u003d addAttribute(CharTermAttribute.class);\n      private final OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        T obj \u003d iter.next();\n        if (obj \u003d\u003d null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term \u003d obj.toString();\n        clearAttributes();\n        termAtt.setEmpty().append(term);\n        offsetAtt.setOffset(start, start+termAtt.length());\n        start +\u003d term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }",
      "path": "lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 273,
      "functionName": "keywordTokenStream",
      "diff": "@@ -1,28 +1,28 @@\n   public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n     // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n     if (keywords \u003d\u003d null)\n       throw new IllegalArgumentException(\"keywords must not be null\");\n     \n     return new TokenStream() {\n       private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n       private int start \u003d 0;\n-      private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n-      private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n+      private final CharTermAttribute termAtt \u003d addAttribute(CharTermAttribute.class);\n+      private final OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n       \n       @Override\n       public boolean incrementToken() {\n         if (!iter.hasNext()) return false;\n         \n         T obj \u003d iter.next();\n         if (obj \u003d\u003d null) \n           throw new IllegalArgumentException(\"keyword must not be null\");\n         \n         String term \u003d obj.toString();\n         clearAttributes();\n-        termAtt.setTermBuffer(term);\n-        offsetAtt.setOffset(start, start+termAtt.termLength());\n+        termAtt.setEmpty().append(term);\n+        offsetAtt.setOffset(start, start+termAtt.length());\n         start +\u003d term.length() + 1; // separate words by 1 (blank) character\n         return true;\n       }\n     };\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "778d96752fa94636a2136ea2b4d58a3fcbe283ec": {
      "type": "Yfilerename",
      "commitMessage": "SVN-GIT conversion, path copy emulation.\n",
      "commitDate": "2016-01-22, 7:18 p.m.",
      "commitName": "778d96752fa94636a2136ea2b4d58a3fcbe283ec",
      "commitAuthor": "Dawid Weiss",
      "commitDateOld": "2010-03-17, 10:57 a.m.",
      "commitNameOld": "2e5c6cdadc820220f8cb86e1b6e215da941649f9",
      "commitAuthorOld": "Uwe Schindler",
      "daysBetweenCommits": 2137.39,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "actualSource": "  public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n    if (keywords \u003d\u003d null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n      private int start \u003d 0;\n      private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        T obj \u003d iter.next();\n        if (obj \u003d\u003d null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term \u003d obj.toString();\n        clearAttributes();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start +\u003d term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }",
      "path": "lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 271,
      "functionName": "keywordTokenStream",
      "diff": "",
      "extendedDetails": {
        "oldPath": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
        "newPath": "lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java"
      }
    },
    "3f722b66a55ace117de6458f5b7d52f5bbc20c62": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-2211: Fix various missing clearAttributes() and improve BaseTokenStreamTestCase to check for this trap\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@899627 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2010-01-15, 8:42 a.m.",
      "commitName": "3f722b66a55ace117de6458f5b7d52f5bbc20c62",
      "commitAuthor": "Uwe Schindler",
      "commitDateOld": "2009-12-05, 4:43 a.m.",
      "commitNameOld": "fa65d42e942dda48fcdb0a7c87c6b5f71f71bb83",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 41.17,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "actualSource": "  public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n    if (keywords \u003d\u003d null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n      private int start \u003d 0;\n      private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        T obj \u003d iter.next();\n        if (obj \u003d\u003d null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term \u003d obj.toString();\n        clearAttributes();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start +\u003d term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 270,
      "functionName": "keywordTokenStream",
      "diff": "@@ -1,27 +1,28 @@\n   public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n     // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n     if (keywords \u003d\u003d null)\n       throw new IllegalArgumentException(\"keywords must not be null\");\n     \n     return new TokenStream() {\n       private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n       private int start \u003d 0;\n       private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n       private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n       \n       @Override\n       public boolean incrementToken() {\n         if (!iter.hasNext()) return false;\n         \n         T obj \u003d iter.next();\n         if (obj \u003d\u003d null) \n           throw new IllegalArgumentException(\"keyword must not be null\");\n         \n         String term \u003d obj.toString();\n+        clearAttributes();\n         termAtt.setTermBuffer(term);\n         offsetAtt.setOffset(start, start+termAtt.termLength());\n         start +\u003d term.length() + 1; // separate words by 1 (blank) character\n         return true;\n       }\n     };\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "786eb6ce0d19c6459f204b5d4ab0dc72245888cb": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-2012: add remaining @overrides (contrib,demo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@833867 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2009-11-08, 7:45 a.m.",
      "commitName": "786eb6ce0d19c6459f204b5d4ab0dc72245888cb",
      "commitAuthor": "Robert Muir",
      "commitDateOld": "2009-11-04, 5:37 p.m.",
      "commitNameOld": "80e8bfbbc9e4932d557d016a4f566f40459cb50d",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 3.59,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "actualSource": "  public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n    if (keywords \u003d\u003d null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n      private int start \u003d 0;\n      private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        T obj \u003d iter.next();\n        if (obj \u003d\u003d null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term \u003d obj.toString();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start +\u003d term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 270,
      "functionName": "keywordTokenStream",
      "diff": "@@ -1,26 +1,27 @@\n   public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n     // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n     if (keywords \u003d\u003d null)\n       throw new IllegalArgumentException(\"keywords must not be null\");\n     \n     return new TokenStream() {\n       private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n       private int start \u003d 0;\n       private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n       private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n       \n+      @Override\n       public boolean incrementToken() {\n         if (!iter.hasNext()) return false;\n         \n         T obj \u003d iter.next();\n         if (obj \u003d\u003d null) \n           throw new IllegalArgumentException(\"keyword must not be null\");\n         \n         String term \u003d obj.toString();\n         termAtt.setTermBuffer(term);\n         offsetAtt.setOffset(start, start+termAtt.termLength());\n         start +\u003d term.length() + 1; // separate words by 1 (blank) character\n         return true;\n       }\n     };\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "786457c0e3ae6afa610788a9ffdfe6bf01c1f976": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "LUCENE-1257: Generics in contrib/memory, contrib/wordnet (previously memory), contrib/misc, contrib/benchmark\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@830790 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2009-10-28, 7:21 p.m.",
      "commitName": "786457c0e3ae6afa610788a9ffdfe6bf01c1f976",
      "commitAuthor": "Uwe Schindler",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "LUCENE-1257: Generics in contrib/memory, contrib/wordnet (previously memory), contrib/misc, contrib/benchmark\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@830790 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2009-10-28, 7:21 p.m.",
          "commitName": "786457c0e3ae6afa610788a9ffdfe6bf01c1f976",
          "commitAuthor": "Uwe Schindler",
          "commitDateOld": "2009-10-26, 10:55 a.m.",
          "commitNameOld": "74f872182ed9699d0e1d16ca98ecf72d7622cb67",
          "commitAuthorOld": "Michael McCandless",
          "daysBetweenCommits": 2.35,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "actualSource": "  public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n    if (keywords \u003d\u003d null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n      private int start \u003d 0;\n      private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        T obj \u003d iter.next();\n        if (obj \u003d\u003d null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term \u003d obj.toString();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start +\u003d term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }",
          "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
          "functionStartLine": 273,
          "functionName": "keywordTokenStream",
          "diff": "@@ -1,26 +1,26 @@\n-  public TokenStream keywordTokenStream(final Collection keywords) {\n+  public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n     // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n     if (keywords \u003d\u003d null)\n       throw new IllegalArgumentException(\"keywords must not be null\");\n     \n     return new TokenStream() {\n-      private Iterator iter \u003d keywords.iterator();\n+      private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n       private int start \u003d 0;\n       private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n       private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n       \n       public boolean incrementToken() {\n         if (!iter.hasNext()) return false;\n         \n-        Object obj \u003d iter.next();\n+        T obj \u003d iter.next();\n         if (obj \u003d\u003d null) \n           throw new IllegalArgumentException(\"keyword must not be null\");\n         \n         String term \u003d obj.toString();\n         termAtt.setTermBuffer(term);\n         offsetAtt.setOffset(start, start+termAtt.termLength());\n         start +\u003d term.length() + 1; // separate words by 1 (blank) character\n         return true;\n       }\n     };\n   }\n\\ No newline at end of file\n",
          "extendedDetails": {
            "oldValue": "[keywords-Collection(modifiers-final)]",
            "newValue": "[keywords-Collection\u003cT\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "LUCENE-1257: Generics in contrib/memory, contrib/wordnet (previously memory), contrib/misc, contrib/benchmark\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@830790 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2009-10-28, 7:21 p.m.",
          "commitName": "786457c0e3ae6afa610788a9ffdfe6bf01c1f976",
          "commitAuthor": "Uwe Schindler",
          "commitDateOld": "2009-10-26, 10:55 a.m.",
          "commitNameOld": "74f872182ed9699d0e1d16ca98ecf72d7622cb67",
          "commitAuthorOld": "Michael McCandless",
          "daysBetweenCommits": 2.35,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "actualSource": "  public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n    if (keywords \u003d\u003d null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n      private int start \u003d 0;\n      private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        T obj \u003d iter.next();\n        if (obj \u003d\u003d null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term \u003d obj.toString();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start +\u003d term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }",
          "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
          "functionStartLine": 273,
          "functionName": "keywordTokenStream",
          "diff": "@@ -1,26 +1,26 @@\n-  public TokenStream keywordTokenStream(final Collection keywords) {\n+  public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n     // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n     if (keywords \u003d\u003d null)\n       throw new IllegalArgumentException(\"keywords must not be null\");\n     \n     return new TokenStream() {\n-      private Iterator iter \u003d keywords.iterator();\n+      private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n       private int start \u003d 0;\n       private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n       private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n       \n       public boolean incrementToken() {\n         if (!iter.hasNext()) return false;\n         \n-        Object obj \u003d iter.next();\n+        T obj \u003d iter.next();\n         if (obj \u003d\u003d null) \n           throw new IllegalArgumentException(\"keyword must not be null\");\n         \n         String term \u003d obj.toString();\n         termAtt.setTermBuffer(term);\n         offsetAtt.setOffset(start, start+termAtt.termLength());\n         start +\u003d term.length() + 1; // separate words by 1 (blank) character\n         return true;\n       }\n     };\n   }\n\\ No newline at end of file\n",
          "extendedDetails": {}
        }
      ]
    },
    "ec90bc2202ef501e257eaf235be5ca15239c03c2": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-1855: Change AttributeSource API to use generics\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@820553 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2009-10-01, 3:53 a.m.",
      "commitName": "ec90bc2202ef501e257eaf235be5ca15239c03c2",
      "commitAuthor": "Uwe Schindler",
      "commitDateOld": "2009-08-26, 7:14 p.m.",
      "commitNameOld": "f39dadfb26311d99b266affd8eb62456f4fbeb9f",
      "commitAuthorOld": "Mark Robert Miller",
      "daysBetweenCommits": 35.36,
      "commitsBetweenForRepo": 105,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n    if (keywords \u003d\u003d null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter \u003d keywords.iterator();\n      private int start \u003d 0;\n      private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        Object obj \u003d iter.next();\n        if (obj \u003d\u003d null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term \u003d obj.toString();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start +\u003d term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 271,
      "functionName": "keywordTokenStream",
      "diff": "@@ -1,26 +1,26 @@\n   public TokenStream keywordTokenStream(final Collection keywords) {\n     // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n     if (keywords \u003d\u003d null)\n       throw new IllegalArgumentException(\"keywords must not be null\");\n     \n     return new TokenStream() {\n       private Iterator iter \u003d keywords.iterator();\n       private int start \u003d 0;\n-      private TermAttribute termAtt \u003d (TermAttribute) addAttribute(TermAttribute.class);\n-      private OffsetAttribute offsetAtt \u003d (OffsetAttribute) addAttribute(OffsetAttribute.class);\n+      private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n+      private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n       \n       public boolean incrementToken() {\n         if (!iter.hasNext()) return false;\n         \n         Object obj \u003d iter.next();\n         if (obj \u003d\u003d null) \n           throw new IllegalArgumentException(\"keyword must not be null\");\n         \n         String term \u003d obj.toString();\n         termAtt.setTermBuffer(term);\n         offsetAtt.setOffset(start, start+termAtt.termLength());\n         start +\u003d term.length() + 1; // separate words by 1 (blank) character\n         return true;\n       }\n     };\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "1743081b078a3206e676bdd4ebe9203f5bad6c90": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-1460: Changed TokenStreams/TokenFilters in contrib to use the new TokenStream API.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@799953 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2009-08-01, 6:52 p.m.",
      "commitName": "1743081b078a3206e676bdd4ebe9203f5bad6c90",
      "commitAuthor": "Michael Busch",
      "commitDateOld": "2009-01-25, 9:38 a.m.",
      "commitNameOld": "c6f6f016433ffe8450e4b2f202c9ac5b848305ec",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 188.34,
      "commitsBetweenForRepo": 413,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n    if (keywords \u003d\u003d null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter \u003d keywords.iterator();\n      private int start \u003d 0;\n      private TermAttribute termAtt \u003d (TermAttribute) addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt \u003d (OffsetAttribute) addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        Object obj \u003d iter.next();\n        if (obj \u003d\u003d null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term \u003d obj.toString();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start +\u003d term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 271,
      "functionName": "keywordTokenStream",
      "diff": "@@ -1,23 +1,26 @@\n   public TokenStream keywordTokenStream(final Collection keywords) {\n     // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n     if (keywords \u003d\u003d null)\n       throw new IllegalArgumentException(\"keywords must not be null\");\n     \n     return new TokenStream() {\n       private Iterator iter \u003d keywords.iterator();\n       private int start \u003d 0;\n-      public Token next(final Token reusableToken) {\n-        assert reusableToken !\u003d null;\n-        if (!iter.hasNext()) return null;\n+      private TermAttribute termAtt \u003d (TermAttribute) addAttribute(TermAttribute.class);\n+      private OffsetAttribute offsetAtt \u003d (OffsetAttribute) addAttribute(OffsetAttribute.class);\n+      \n+      public boolean incrementToken() {\n+        if (!iter.hasNext()) return false;\n         \n         Object obj \u003d iter.next();\n         if (obj \u003d\u003d null) \n           throw new IllegalArgumentException(\"keyword must not be null\");\n         \n         String term \u003d obj.toString();\n-        reusableToken.reinit(term, start, start+reusableToken.termLength());\n+        termAtt.setTermBuffer(term);\n+        offsetAtt.setOffset(start, start+termAtt.termLength());\n         start +\u003d term.length() + 1; // separate words by 1 (blank) character\n-        return reusableToken;\n+        return true;\n       }\n     };\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "bb6b7117186b656b4777850fdc463e0eaa541130": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-1333: improvements to Token reuse API and full cutover to reuse API for all core and contrib analyzers\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@687357 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2008-08-20, 10:38 a.m.",
      "commitName": "bb6b7117186b656b4777850fdc463e0eaa541130",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2008-08-19, 6:40 a.m.",
      "commitNameOld": "e31a9da835c6e381bbcd46aa4390089f6bcc1c15",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 1.16,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n    if (keywords \u003d\u003d null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter \u003d keywords.iterator();\n      private int start \u003d 0;\n      public Token next(final Token reusableToken) {\n        assert reusableToken !\u003d null;\n        if (!iter.hasNext()) return null;\n        \n        Object obj \u003d iter.next();\n        if (obj \u003d\u003d null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term \u003d obj.toString();\n        reusableToken.reinit(term, start, start+reusableToken.termLength());\n        start +\u003d term.length() + 1; // separate words by 1 (blank) character\n        return reusableToken;\n      }\n    };\n  }",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 270,
      "functionName": "keywordTokenStream",
      "diff": "@@ -1,22 +1,23 @@\n   public TokenStream keywordTokenStream(final Collection keywords) {\n     // TODO: deprecate \u0026 move this method into AnalyzerUtil?\n     if (keywords \u003d\u003d null)\n       throw new IllegalArgumentException(\"keywords must not be null\");\n     \n     return new TokenStream() {\n       private Iterator iter \u003d keywords.iterator();\n       private int start \u003d 0;\n-      public Token next() {\n+      public Token next(final Token reusableToken) {\n+        assert reusableToken !\u003d null;\n         if (!iter.hasNext()) return null;\n         \n         Object obj \u003d iter.next();\n         if (obj \u003d\u003d null) \n           throw new IllegalArgumentException(\"keyword must not be null\");\n         \n         String term \u003d obj.toString();\n-        Token token \u003d new Token(term, start, start + term.length());\n+        reusableToken.reinit(term, start, start+reusableToken.termLength());\n         start +\u003d term.length() + 1; // separate words by 1 (blank) character\n-        return token;\n+        return reusableToken;\n       }\n     };\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "e28541354d496a43078c1bc281076f97ed7d008c": {
      "type": "Yintroduced",
      "commitMessage": "some performance improvements\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@351891 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2005-12-03, 12:24 a.m.",
      "commitName": "e28541354d496a43078c1bc281076f97ed7d008c",
      "commitAuthor": "Wolfgang Hoschek",
      "diff": "@@ -0,0 +1,22 @@\n+\tpublic TokenStream keywordTokenStream(final Collection keywords) {\n+\t\t// TODO: deprecate \u0026 move this method into AnalyzerUtil?\n+\t\tif (keywords \u003d\u003d null)\n+\t\t\tthrow new IllegalArgumentException(\"keywords must not be null\");\n+\t\t\n+\t\treturn new TokenStream() {\n+\t\t\tprivate Iterator iter \u003d keywords.iterator();\n+\t\t\tprivate int start \u003d 0;\n+\t\t\tpublic Token next() {\n+\t\t\t\tif (!iter.hasNext()) return null;\n+\t\t\t\t\n+\t\t\t\tObject obj \u003d iter.next();\n+\t\t\t\tif (obj \u003d\u003d null) \n+\t\t\t\t\tthrow new IllegalArgumentException(\"keyword must not be null\");\n+\t\t\t\t\n+\t\t\t\tString term \u003d obj.toString();\n+\t\t\t\tToken token \u003d new Token(term, start, start + term.length());\n+\t\t\t\tstart +\u003d term.length() + 1; // separate words by 1 (blank) character\n+\t\t\t\treturn token;\n+\t\t\t}\n+\t\t};\n+\t}\n\\ No newline at end of file\n",
      "actualSource": "\tpublic TokenStream keywordTokenStream(final Collection keywords) {\n\t\t// TODO: deprecate \u0026 move this method into AnalyzerUtil?\n\t\tif (keywords \u003d\u003d null)\n\t\t\tthrow new IllegalArgumentException(\"keywords must not be null\");\n\t\t\n\t\treturn new TokenStream() {\n\t\t\tprivate Iterator iter \u003d keywords.iterator();\n\t\t\tprivate int start \u003d 0;\n\t\t\tpublic Token next() {\n\t\t\t\tif (!iter.hasNext()) return null;\n\t\t\t\t\n\t\t\t\tObject obj \u003d iter.next();\n\t\t\t\tif (obj \u003d\u003d null) \n\t\t\t\t\tthrow new IllegalArgumentException(\"keyword must not be null\");\n\t\t\t\t\n\t\t\t\tString term \u003d obj.toString();\n\t\t\t\tToken token \u003d new Token(term, start, start + term.length());\n\t\t\t\tstart +\u003d term.length() + 1; // separate words by 1 (blank) character\n\t\t\t\treturn token;\n\t\t\t}\n\t\t};\n\t}",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 252,
      "functionName": "keywordTokenStream"
    }
  }
}