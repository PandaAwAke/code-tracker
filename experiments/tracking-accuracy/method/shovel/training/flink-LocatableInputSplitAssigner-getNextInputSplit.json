{
  "origin": "codeshovel",
  "repositoryName": "flink",
  "repositoryPath": "H:\\Projects\\apache\\flink/.git",
  "startCommitName": "9e936a5f8198b0059e9b5fba33163c2bbe3efbdd",
  "sourceFileName": "LocatableInputSplitAssigner.java",
  "functionName": "getNextInputSplit",
  "functionId": "getNextInputSplit___host-String__taskId-int",
  "sourceFilePath": "flink-core/src/main/java/org/apache/flink/api/common/io/LocatableInputSplitAssigner.java",
  "functionStartLine": 76,
  "functionEndLine": 203,
  "numCommitsSeen": 31,
  "timeTaken": 3147,
  "changeHistory": [
    "7452802bc25c0915b7347d4faf2d60adcfc27644",
    "e0a4ee07084bc6ab56a20fbc4a18863462da93eb",
    "a959dd5034127161aafcf9c56222c7d08aa80e54",
    "b6f599f1ed27a28ee0f8be7176f06a5fa43fa310",
    "b32e77a2d8be76aeafa28b94fd7cfbb8de80f4cb",
    "c32569aed12ffa968e2c2289c2d56db262c0eba4",
    "8563d511da8ab8ac0e1362775f11aef7b67375be",
    "e73296f3e3fad9bb715edd0ff7a0eb10ce1226b9",
    "33cb2ca9898809d2fc90765996ea56bbea458e59",
    "b407f8b37a4ded820a25cc1cc03da064988c2f0d",
    "6651d55a106c85be53373dd3877900f1ffc80793",
    "1e17928631239d0f546e37069929170cab8363ff"
  ],
  "changeHistoryShort": {
    "7452802bc25c0915b7347d4faf2d60adcfc27644": "Yparameterchange",
    "e0a4ee07084bc6ab56a20fbc4a18863462da93eb": "Ybodychange",
    "a959dd5034127161aafcf9c56222c7d08aa80e54": "Ybodychange",
    "b6f599f1ed27a28ee0f8be7176f06a5fa43fa310": "Ybodychange",
    "b32e77a2d8be76aeafa28b94fd7cfbb8de80f4cb": "Yfilerename",
    "c32569aed12ffa968e2c2289c2d56db262c0eba4": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "8563d511da8ab8ac0e1362775f11aef7b67375be": "Yfilerename",
    "e73296f3e3fad9bb715edd0ff7a0eb10ce1226b9": "Ymultichange(Ymovefromfile,Ybodychange)",
    "33cb2ca9898809d2fc90765996ea56bbea458e59": "Yfilerename",
    "b407f8b37a4ded820a25cc1cc03da064988c2f0d": "Ybodychange",
    "6651d55a106c85be53373dd3877900f1ffc80793": "Ybodychange",
    "1e17928631239d0f546e37069929170cab8363ff": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7452802bc25c0915b7347d4faf2d60adcfc27644": {
      "type": "Yparameterchange",
      "commitMessage": "[FLINK-1443 [api-breaking] Extended split assigner interface by parallel task id.\n",
      "commitDate": "2015-02-05, 5:18 a.m.",
      "commitName": "7452802bc25c0915b7347d4faf2d60adcfc27644",
      "commitAuthor": "Fabian Hueske",
      "commitDateOld": "2014-12-12, 9:53 a.m.",
      "commitNameOld": "e0a4ee07084bc6ab56a20fbc4a18863462da93eb",
      "commitAuthorOld": "Fabian Hueske",
      "daysBetweenCommits": 54.81,
      "commitsBetweenForRepo": 284,
      "commitsBetweenForFile": 1,
      "actualSource": "\tpublic LocatableInputSplit getNextInputSplit(String host, int taskId) {\n\n\t\t// for a null host, we return a remote split\n\t\tif (host \u003d\u003d null) {\n\t\t\tsynchronized (this.remoteSplitChooser) {\n\t\t\t\tsynchronized (this.unassigned) {\n\n\t\t\t\t\tLocatableInputSplitWithCount split \u003d this.remoteSplitChooser.getNextUnassignedMinLocalCountSplit(this.unassigned);\n\n\t\t\t\t\tif (split !\u003d null) {\n\t\t\t\t\t\t// got a split to assign. Double check that it hasn\u0027t been assigned before.\n\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tremoteAssignments++;\n\t\t\t\t\t\t\treturn split.getSplit();\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// all splits consumed\n\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn null;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\thost \u003d host.toLowerCase(Locale.US);\n\n\t\t// for any non-null host, we take the list of non-null splits\n\t\tLocatableInputSplitChooser localSplits \u003d this.localPerHost.get(host);\n\n\t\t// if we have no list for this host yet, create one\n\t\tif (localSplits \u003d\u003d null) {\n\t\t\tlocalSplits \u003d new LocatableInputSplitChooser();\n\n\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n\t\t\tsynchronized (localSplits) {\n\t\t\t\tLocatableInputSplitChooser prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t\t// simply work with that other list\n\t\t\t\tif (prior \u003d\u003d null) {\n\t\t\t\t\t// we are the first, we populate\n\n\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t\t// because that is shared among threads\n\t\t\t\t\tLocatableInputSplitWithCount[] remaining;\n\t\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\t\tremaining \u003d this.unassigned.toArray(new LocatableInputSplitWithCount[this.unassigned.size()]);\n\t\t\t\t\t}\n\n\t\t\t\t\tfor (LocatableInputSplitWithCount isw : remaining) {\n\t\t\t\t\t\tif (isLocal(host, isw.getSplit().getHostnames())) {\n\t\t\t\t\t\t\t// Split is local on host.\n\t\t\t\t\t\t\t// Increment local count\n\t\t\t\t\t\t\tisw.incrementLocalCount();\n                            // and add to local split list\n\t\t\t\t\t\t\tlocalSplits.addInputSplit(isw);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// someone else was faster\n\t\t\t\t\tlocalSplits \u003d prior;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\n\t\t// at this point, we have a list of local splits (possibly empty)\n\t\t// we need to make sure no one else operates in the current list (that protects against\n\t\t// list creation races) and that the unassigned set is consistent\n\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\t\tsynchronized (localSplits) {\n\t\t\tsynchronized (this.unassigned) {\n\n\t\t\t\tLocatableInputSplitWithCount split \u003d localSplits.getNextUnassignedMinLocalCountSplit(this.unassigned);\n\n\t\t\t\tif (split !\u003d null) {\n\t\t\t\t\t// found a valid split. Double check that it hasn\u0027t been assigned before.\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split.getSplit();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// we did not find a local split, return a remote split\n\t\tsynchronized (this.remoteSplitChooser) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tLocatableInputSplitWithCount split \u003d this.remoteSplitChooser.getNextUnassignedMinLocalCountSplit(this.unassigned);\n\n\t\t\t\tif (split !\u003d null) {\n\t\t\t\t\t// found a valid split. Double check that it hasn\u0027t been assigned yet.\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tremoteAssignments++;\n\t\t\t\t\t\treturn split.getSplit();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// all splits consumed\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}",
      "path": "flink-core/src/main/java/org/apache/flink/api/common/io/LocatableInputSplitAssigner.java",
      "functionStartLine": 74,
      "functionName": "getNextInputSplit",
      "diff": "@@ -1,128 +1,128 @@\n-\tpublic LocatableInputSplit getNextInputSplit(String host) {\n+\tpublic LocatableInputSplit getNextInputSplit(String host, int taskId) {\n \n \t\t// for a null host, we return a remote split\n \t\tif (host \u003d\u003d null) {\n \t\t\tsynchronized (this.remoteSplitChooser) {\n \t\t\t\tsynchronized (this.unassigned) {\n \n \t\t\t\t\tLocatableInputSplitWithCount split \u003d this.remoteSplitChooser.getNextUnassignedMinLocalCountSplit(this.unassigned);\n \n \t\t\t\t\tif (split !\u003d null) {\n \t\t\t\t\t\t// got a split to assign. Double check that it hasn\u0027t been assigned before.\n \t\t\t\t\t\tif (this.unassigned.remove(split)) {\n \t\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n \t\t\t\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n \t\t\t\t\t\t\t}\n \n \t\t\t\t\t\t\tremoteAssignments++;\n \t\t\t\t\t\t\treturn split.getSplit();\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n \t\t\t\t\t\t}\n \t\t\t\t\t} else {\n \t\t\t\t\t\t// all splits consumed\n \t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n \t\t\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n \t\t\t\t\t\t}\n \t\t\t\t\t\treturn null;\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n \t\thost \u003d host.toLowerCase(Locale.US);\n \n \t\t// for any non-null host, we take the list of non-null splits\n \t\tLocatableInputSplitChooser localSplits \u003d this.localPerHost.get(host);\n \n \t\t// if we have no list for this host yet, create one\n \t\tif (localSplits \u003d\u003d null) {\n \t\t\tlocalSplits \u003d new LocatableInputSplitChooser();\n \n \t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n \t\t\tsynchronized (localSplits) {\n \t\t\t\tLocatableInputSplitChooser prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n \n \t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n \t\t\t\t// simply work with that other list\n \t\t\t\tif (prior \u003d\u003d null) {\n \t\t\t\t\t// we are the first, we populate\n \n \t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n \t\t\t\t\t// because that is shared among threads\n \t\t\t\t\tLocatableInputSplitWithCount[] remaining;\n \t\t\t\t\tsynchronized (this.unassigned) {\n \t\t\t\t\t\tremaining \u003d this.unassigned.toArray(new LocatableInputSplitWithCount[this.unassigned.size()]);\n \t\t\t\t\t}\n \n \t\t\t\t\tfor (LocatableInputSplitWithCount isw : remaining) {\n \t\t\t\t\t\tif (isLocal(host, isw.getSplit().getHostnames())) {\n \t\t\t\t\t\t\t// Split is local on host.\n \t\t\t\t\t\t\t// Increment local count\n \t\t\t\t\t\t\tisw.incrementLocalCount();\n                             // and add to local split list\n \t\t\t\t\t\t\tlocalSplits.addInputSplit(isw);\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n \n \t\t\t\t}\n \t\t\t\telse {\n \t\t\t\t\t// someone else was faster\n \t\t\t\t\tlocalSplits \u003d prior;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n \n \t\t// at this point, we have a list of local splits (possibly empty)\n \t\t// we need to make sure no one else operates in the current list (that protects against\n \t\t// list creation races) and that the unassigned set is consistent\n \t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n \t\tsynchronized (localSplits) {\n \t\t\tsynchronized (this.unassigned) {\n \n \t\t\t\tLocatableInputSplitWithCount split \u003d localSplits.getNextUnassignedMinLocalCountSplit(this.unassigned);\n \n \t\t\t\tif (split !\u003d null) {\n \t\t\t\t\t// found a valid split. Double check that it hasn\u0027t been assigned before.\n \t\t\t\t\tif (this.unassigned.remove(split)) {\n \t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n \t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n \t\t\t\t\t\t}\n \n \t\t\t\t\t\tlocalAssignments++;\n \t\t\t\t\t\treturn split.getSplit();\n \t\t\t\t\t} else {\n \t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n \t\t// we did not find a local split, return a remote split\n \t\tsynchronized (this.remoteSplitChooser) {\n \t\t\tsynchronized (this.unassigned) {\n \t\t\t\tLocatableInputSplitWithCount split \u003d this.remoteSplitChooser.getNextUnassignedMinLocalCountSplit(this.unassigned);\n \n \t\t\t\tif (split !\u003d null) {\n \t\t\t\t\t// found a valid split. Double check that it hasn\u0027t been assigned yet.\n \t\t\t\t\tif (this.unassigned.remove(split)) {\n \t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n \t\t\t\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n \t\t\t\t\t\t}\n \n \t\t\t\t\t\tremoteAssignments++;\n \t\t\t\t\t\treturn split.getSplit();\n \t\t\t\t\t} else {\n \t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n \t\t\t\t\t}\n \t\t\t\t} else {\n \t\t\t\t\t// all splits consumed\n \t\t\t\t\tif (LOG.isDebugEnabled()) {\n \t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n \t\t\t\t\t}\n \t\t\t\t\treturn null;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \t}\n\\ No newline at end of file\n",
      "extendedDetails": {
        "oldValue": "[host-String]",
        "newValue": "[host-String, taskId-int]"
      }
    },
    "e0a4ee07084bc6ab56a20fbc4a18863462da93eb": {
      "type": "Ybodychange",
      "commitMessage": "[FLINK-1287] LocalizableSplitAssigner prefers splits with less degrees of freedom\n\nThis closes #258\n",
      "commitDate": "2014-12-12, 9:53 a.m.",
      "commitName": "e0a4ee07084bc6ab56a20fbc4a18863462da93eb",
      "commitAuthor": "Fabian Hueske",
      "commitDateOld": "2014-11-06, 8:00 a.m.",
      "commitNameOld": "a959dd5034127161aafcf9c56222c7d08aa80e54",
      "commitAuthorOld": "Stephan Ewen",
      "daysBetweenCommits": 36.08,
      "commitsBetweenForRepo": 166,
      "commitsBetweenForFile": 1,
      "actualSource": "\tpublic LocatableInputSplit getNextInputSplit(String host) {\n\n\t\t// for a null host, we return a remote split\n\t\tif (host \u003d\u003d null) {\n\t\t\tsynchronized (this.remoteSplitChooser) {\n\t\t\t\tsynchronized (this.unassigned) {\n\n\t\t\t\t\tLocatableInputSplitWithCount split \u003d this.remoteSplitChooser.getNextUnassignedMinLocalCountSplit(this.unassigned);\n\n\t\t\t\t\tif (split !\u003d null) {\n\t\t\t\t\t\t// got a split to assign. Double check that it hasn\u0027t been assigned before.\n\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tremoteAssignments++;\n\t\t\t\t\t\t\treturn split.getSplit();\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// all splits consumed\n\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn null;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\thost \u003d host.toLowerCase(Locale.US);\n\n\t\t// for any non-null host, we take the list of non-null splits\n\t\tLocatableInputSplitChooser localSplits \u003d this.localPerHost.get(host);\n\n\t\t// if we have no list for this host yet, create one\n\t\tif (localSplits \u003d\u003d null) {\n\t\t\tlocalSplits \u003d new LocatableInputSplitChooser();\n\n\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n\t\t\tsynchronized (localSplits) {\n\t\t\t\tLocatableInputSplitChooser prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t\t// simply work with that other list\n\t\t\t\tif (prior \u003d\u003d null) {\n\t\t\t\t\t// we are the first, we populate\n\n\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t\t// because that is shared among threads\n\t\t\t\t\tLocatableInputSplitWithCount[] remaining;\n\t\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\t\tremaining \u003d this.unassigned.toArray(new LocatableInputSplitWithCount[this.unassigned.size()]);\n\t\t\t\t\t}\n\n\t\t\t\t\tfor (LocatableInputSplitWithCount isw : remaining) {\n\t\t\t\t\t\tif (isLocal(host, isw.getSplit().getHostnames())) {\n\t\t\t\t\t\t\t// Split is local on host.\n\t\t\t\t\t\t\t// Increment local count\n\t\t\t\t\t\t\tisw.incrementLocalCount();\n                            // and add to local split list\n\t\t\t\t\t\t\tlocalSplits.addInputSplit(isw);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// someone else was faster\n\t\t\t\t\tlocalSplits \u003d prior;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\n\t\t// at this point, we have a list of local splits (possibly empty)\n\t\t// we need to make sure no one else operates in the current list (that protects against\n\t\t// list creation races) and that the unassigned set is consistent\n\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\t\tsynchronized (localSplits) {\n\t\t\tsynchronized (this.unassigned) {\n\n\t\t\t\tLocatableInputSplitWithCount split \u003d localSplits.getNextUnassignedMinLocalCountSplit(this.unassigned);\n\n\t\t\t\tif (split !\u003d null) {\n\t\t\t\t\t// found a valid split. Double check that it hasn\u0027t been assigned before.\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split.getSplit();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// we did not find a local split, return a remote split\n\t\tsynchronized (this.remoteSplitChooser) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tLocatableInputSplitWithCount split \u003d this.remoteSplitChooser.getNextUnassignedMinLocalCountSplit(this.unassigned);\n\n\t\t\t\tif (split !\u003d null) {\n\t\t\t\t\t// found a valid split. Double check that it hasn\u0027t been assigned yet.\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tremoteAssignments++;\n\t\t\t\t\t\treturn split.getSplit();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// all splits consumed\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}",
      "path": "flink-core/src/main/java/org/apache/flink/api/common/io/LocatableInputSplitAssigner.java",
      "functionStartLine": 74,
      "functionName": "getNextInputSplit",
      "diff": "@@ -1,110 +1,128 @@\n \tpublic LocatableInputSplit getNextInputSplit(String host) {\n-\t\t// for a null host, we return an arbitrary split\n+\n+\t\t// for a null host, we return a remote split\n \t\tif (host \u003d\u003d null) {\n-\t\t\t\n-\t\t\tsynchronized (this.unassigned) {\n-\t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n-\t\t\t\tif (iter.hasNext()) {\n-\t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n-\t\t\t\t\titer.remove();\n-\t\t\t\t\t\n-\t\t\t\t\tif (LOG.isInfoEnabled()) {\n-\t\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n+\t\t\tsynchronized (this.remoteSplitChooser) {\n+\t\t\t\tsynchronized (this.unassigned) {\n+\n+\t\t\t\t\tLocatableInputSplitWithCount split \u003d this.remoteSplitChooser.getNextUnassignedMinLocalCountSplit(this.unassigned);\n+\n+\t\t\t\t\tif (split !\u003d null) {\n+\t\t\t\t\t\t// got a split to assign. Double check that it hasn\u0027t been assigned before.\n+\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n+\t\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n+\t\t\t\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n+\t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t\tremoteAssignments++;\n+\t\t\t\t\t\t\treturn split.getSplit();\n+\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t// all splits consumed\n+\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\treturn null;\n \t\t\t\t\t}\n-\t\t\t\t\t\n-\t\t\t\t\tremoteAssignments++;\n-\t\t\t\t\treturn next;\n-\t\t\t\t} else {\n-\t\t\t\t\tif (LOG.isDebugEnabled()) {\n-\t\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n-\t\t\t\t\t}\n-\t\t\t\t\treturn null;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n-\t\t\n+\n \t\thost \u003d host.toLowerCase(Locale.US);\n-\t\t\n+\n \t\t// for any non-null host, we take the list of non-null splits\n-\t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n-\t\t\n+\t\tLocatableInputSplitChooser localSplits \u003d this.localPerHost.get(host);\n+\n \t\t// if we have no list for this host yet, create one\n \t\tif (localSplits \u003d\u003d null) {\n-\t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n-\t\t\t\n+\t\t\tlocalSplits \u003d new LocatableInputSplitChooser();\n+\n \t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n \t\t\tsynchronized (localSplits) {\n-\t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n-\t\t\t\t\n+\t\t\t\tLocatableInputSplitChooser prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n+\n \t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n \t\t\t\t// simply work with that other list\n \t\t\t\tif (prior \u003d\u003d null) {\n \t\t\t\t\t// we are the first, we populate\n-\t\t\t\t\t\n+\n \t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n \t\t\t\t\t// because that is shared among threads\n-\t\t\t\t\tLocatableInputSplit[] remaining;\n+\t\t\t\t\tLocatableInputSplitWithCount[] remaining;\n \t\t\t\t\tsynchronized (this.unassigned) {\n-\t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n+\t\t\t\t\t\tremaining \u003d this.unassigned.toArray(new LocatableInputSplitWithCount[this.unassigned.size()]);\n \t\t\t\t\t}\n-\t\t\t\t\t\n-\t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n-\t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n-\t\t\t\t\t\t\tlocalSplits.add(is);\n+\n+\t\t\t\t\tfor (LocatableInputSplitWithCount isw : remaining) {\n+\t\t\t\t\t\tif (isLocal(host, isw.getSplit().getHostnames())) {\n+\t\t\t\t\t\t\t// Split is local on host.\n+\t\t\t\t\t\t\t// Increment local count\n+\t\t\t\t\t\t\tisw.incrementLocalCount();\n+                            // and add to local split list\n+\t\t\t\t\t\t\tlocalSplits.addInputSplit(isw);\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n+\n \t\t\t\t}\n \t\t\t\telse {\n \t\t\t\t\t// someone else was faster\n \t\t\t\t\tlocalSplits \u003d prior;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n-\t\t\n-\t\t\n+\n+\n \t\t// at this point, we have a list of local splits (possibly empty)\n \t\t// we need to make sure no one else operates in the current list (that protects against\n \t\t// list creation races) and that the unassigned set is consistent\n \t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n \t\tsynchronized (localSplits) {\n-\t\t\tint size \u003d localSplits.size();\n-\t\t\tif (size \u003e 0) {\n-\t\t\t\tsynchronized (this.unassigned) {\n-\t\t\t\t\tdo {\n-\t\t\t\t\t\t--size;\n-\t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n-\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n-\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n-\t\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\tlocalAssignments++;\n-\t\t\t\t\t\t\treturn split;\n+\t\t\tsynchronized (this.unassigned) {\n+\n+\t\t\t\tLocatableInputSplitWithCount split \u003d localSplits.getNextUnassignedMinLocalCountSplit(this.unassigned);\n+\n+\t\t\t\tif (split !\u003d null) {\n+\t\t\t\t\t// found a valid split. Double check that it hasn\u0027t been assigned before.\n+\t\t\t\t\tif (this.unassigned.remove(split)) {\n+\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n+\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n \t\t\t\t\t\t}\n-\t\t\t\t\t} while (size \u003e 0);\n+\n+\t\t\t\t\t\tlocalAssignments++;\n+\t\t\t\t\t\treturn split.getSplit();\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n-\t\t\n-\t\t// we did not find a local split, return any\n-\t\tsynchronized (this.unassigned) {\n-\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n-\t\t\tif (iter.hasNext()) {\n-\t\t\t\tLocatableInputSplit next \u003d iter.next();\n-\t\t\t\titer.remove();\n-\t\t\t\t\n-\t\t\t\tif (LOG.isInfoEnabled()) {\n-\t\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n+\n+\t\t// we did not find a local split, return a remote split\n+\t\tsynchronized (this.remoteSplitChooser) {\n+\t\t\tsynchronized (this.unassigned) {\n+\t\t\t\tLocatableInputSplitWithCount split \u003d this.remoteSplitChooser.getNextUnassignedMinLocalCountSplit(this.unassigned);\n+\n+\t\t\t\tif (split !\u003d null) {\n+\t\t\t\t\t// found a valid split. Double check that it hasn\u0027t been assigned yet.\n+\t\t\t\t\tif (this.unassigned.remove(split)) {\n+\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n+\t\t\t\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n+\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\tremoteAssignments++;\n+\t\t\t\t\t\treturn split.getSplit();\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tthrow new IllegalStateException(\"Chosen InputSplit has already been assigned. This should not happen!\");\n+\t\t\t\t\t}\n+\t\t\t\t} else {\n+\t\t\t\t\t// all splits consumed\n+\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n+\t\t\t\t\t}\n+\t\t\t\t\treturn null;\n \t\t\t\t}\n-\t\t\t\t\n-\t\t\t\tremoteAssignments++;\n-\t\t\t\treturn next;\n-\t\t\t} else {\n-\t\t\t\tif (LOG.isDebugEnabled()) {\n-\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n-\t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n \t\t}\n \t}\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "a959dd5034127161aafcf9c56222c7d08aa80e54": {
      "type": "Ybodychange",
      "commitMessage": "[FLINK-1220] Promote various log statements to INFO level. Add log statements for assignment locations\n",
      "commitDate": "2014-11-06, 8:00 a.m.",
      "commitName": "a959dd5034127161aafcf9c56222c7d08aa80e54",
      "commitAuthor": "Stephan Ewen",
      "commitDateOld": "2014-10-18, 11:13 a.m.",
      "commitNameOld": "23e30f09d4b94e71164ba9fb71d934ebfaf124d5",
      "commitAuthorOld": "Robert Metzger",
      "daysBetweenCommits": 18.91,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "actualSource": "\tpublic LocatableInputSplit getNextInputSplit(String host) {\n\t\t// for a null host, we return an arbitrary split\n\t\tif (host \u003d\u003d null) {\n\t\t\t\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\t\tif (iter.hasNext()) {\n\t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\t\titer.remove();\n\t\t\t\t\t\n\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tremoteAssignments++;\n\t\t\t\t\treturn next;\n\t\t\t\t} else {\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\thost \u003d host.toLowerCase(Locale.US);\n\t\t\n\t\t// for any non-null host, we take the list of non-null splits\n\t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n\t\t\n\t\t// if we have no list for this host yet, create one\n\t\tif (localSplits \u003d\u003d null) {\n\t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n\t\t\t\n\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n\t\t\tsynchronized (localSplits) {\n\t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n\t\t\t\t\n\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t\t// simply work with that other list\n\t\t\t\tif (prior \u003d\u003d null) {\n\t\t\t\t\t// we are the first, we populate\n\t\t\t\t\t\n\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t\t// because that is shared among threads\n\t\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// someone else was faster\n\t\t\t\t\tlocalSplits \u003d prior;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t\n\t\t// at this point, we have a list of local splits (possibly empty)\n\t\t// we need to make sure no one else operates in the current list (that protects against\n\t\t// list creation races) and that the unassigned set is consistent\n\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\t\tsynchronized (localSplits) {\n\t\t\tint size \u003d localSplits.size();\n\t\t\tif (size \u003e 0) {\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tdo {\n\t\t\t\t\t\t--size;\n\t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\t\treturn split;\n\t\t\t\t\t\t}\n\t\t\t\t\t} while (size \u003e 0);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t// we did not find a local split, return any\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\titer.remove();\n\t\t\t\t\n\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}",
      "path": "flink-core/src/main/java/org/apache/flink/api/common/io/LocatableInputSplitAssigner.java",
      "functionStartLine": 67,
      "functionName": "getNextInputSplit",
      "diff": "@@ -1,110 +1,110 @@\n \tpublic LocatableInputSplit getNextInputSplit(String host) {\n \t\t// for a null host, we return an arbitrary split\n \t\tif (host \u003d\u003d null) {\n \t\t\t\n \t\t\tsynchronized (this.unassigned) {\n \t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n \t\t\t\tif (iter.hasNext()) {\n \t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n \t\t\t\t\titer.remove();\n \t\t\t\t\t\n-\t\t\t\t\tif (LOG.isDebugEnabled()) {\n-\t\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n+\t\t\t\t\tif (LOG.isInfoEnabled()) {\n+\t\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n \t\t\t\t\t}\n \t\t\t\t\t\n \t\t\t\t\tremoteAssignments++;\n \t\t\t\t\treturn next;\n \t\t\t\t} else {\n \t\t\t\t\tif (LOG.isDebugEnabled()) {\n \t\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n \t\t\t\t\t}\n \t\t\t\t\treturn null;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \t\t\n \t\thost \u003d host.toLowerCase(Locale.US);\n \t\t\n \t\t// for any non-null host, we take the list of non-null splits\n \t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n \t\t\n \t\t// if we have no list for this host yet, create one\n \t\tif (localSplits \u003d\u003d null) {\n \t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n \t\t\t\n \t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n \t\t\tsynchronized (localSplits) {\n \t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n \t\t\t\t\n \t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n \t\t\t\t// simply work with that other list\n \t\t\t\tif (prior \u003d\u003d null) {\n \t\t\t\t\t// we are the first, we populate\n \t\t\t\t\t\n \t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n \t\t\t\t\t// because that is shared among threads\n \t\t\t\t\tLocatableInputSplit[] remaining;\n \t\t\t\t\tsynchronized (this.unassigned) {\n \t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n \t\t\t\t\t}\n \t\t\t\t\t\n \t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n \t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n \t\t\t\t\t\t\tlocalSplits.add(is);\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\telse {\n \t\t\t\t\t// someone else was faster\n \t\t\t\t\tlocalSplits \u003d prior;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \t\t\n \t\t\n \t\t// at this point, we have a list of local splits (possibly empty)\n \t\t// we need to make sure no one else operates in the current list (that protects against\n \t\t// list creation races) and that the unassigned set is consistent\n \t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n \t\tsynchronized (localSplits) {\n \t\t\tint size \u003d localSplits.size();\n \t\t\tif (size \u003e 0) {\n \t\t\t\tsynchronized (this.unassigned) {\n \t\t\t\t\tdo {\n \t\t\t\t\t\t--size;\n \t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n \t\t\t\t\t\tif (this.unassigned.remove(split)) {\n \t\t\t\t\t\t\t\n-\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n-\t\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n+\t\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n+\t\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n \t\t\t\t\t\t\t}\n \t\t\t\t\t\t\t\n \t\t\t\t\t\t\tlocalAssignments++;\n \t\t\t\t\t\t\treturn split;\n \t\t\t\t\t\t}\n \t\t\t\t\t} while (size \u003e 0);\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \t\t\n \t\t// we did not find a local split, return any\n \t\tsynchronized (this.unassigned) {\n \t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n \t\t\tif (iter.hasNext()) {\n \t\t\t\tLocatableInputSplit next \u003d iter.next();\n \t\t\t\titer.remove();\n \t\t\t\t\n-\t\t\t\tif (LOG.isDebugEnabled()) {\n-\t\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n+\t\t\t\tif (LOG.isInfoEnabled()) {\n+\t\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\tremoteAssignments++;\n \t\t\t\treturn next;\n \t\t\t} else {\n \t\t\t\tif (LOG.isDebugEnabled()) {\n \t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n \t\t\t\t}\n \t\t\t\treturn null;\n \t\t\t}\n \t\t}\n \t}\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "b6f599f1ed27a28ee0f8be7176f06a5fa43fa310": {
      "type": "Ybodychange",
      "commitMessage": "[FLINK-1170] Fix faulty input split localization\nPass hostname to split assigner\nAvoid clashes by only using the first component of the fully qualified hostname\n",
      "commitDate": "2014-10-18, 11:13 a.m.",
      "commitName": "b6f599f1ed27a28ee0f8be7176f06a5fa43fa310",
      "commitAuthor": "Robert Metzger",
      "commitDateOld": "2014-09-23, 7:59 a.m.",
      "commitNameOld": "b904b0041cf97b2c6181b1985afc457ed01cf626",
      "commitAuthorOld": "Till Rohrmann",
      "daysBetweenCommits": 25.13,
      "commitsBetweenForRepo": 148,
      "commitsBetweenForFile": 1,
      "actualSource": "\tpublic LocatableInputSplit getNextInputSplit(String host) {\n\t\t// for a null host, we return an arbitrary split\n\t\tif (host \u003d\u003d null) {\n\t\t\t\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\t\tif (iter.hasNext()) {\n\t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\t\titer.remove();\n\t\t\t\t\t\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tremoteAssignments++;\n\t\t\t\t\treturn next;\n\t\t\t\t} else {\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\thost \u003d host.toLowerCase(Locale.US);\n\t\t\n\t\t// for any non-null host, we take the list of non-null splits\n\t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n\t\t\n\t\t// if we have no list for this host yet, create one\n\t\tif (localSplits \u003d\u003d null) {\n\t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n\t\t\t\n\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n\t\t\tsynchronized (localSplits) {\n\t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n\t\t\t\t\n\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t\t// simply work with that other list\n\t\t\t\tif (prior \u003d\u003d null) {\n\t\t\t\t\t// we are the first, we populate\n\t\t\t\t\t\n\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t\t// because that is shared among threads\n\t\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// someone else was faster\n\t\t\t\t\tlocalSplits \u003d prior;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t\n\t\t// at this point, we have a list of local splits (possibly empty)\n\t\t// we need to make sure no one else operates in the current list (that protects against\n\t\t// list creation races) and that the unassigned set is consistent\n\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\t\tsynchronized (localSplits) {\n\t\t\tint size \u003d localSplits.size();\n\t\t\tif (size \u003e 0) {\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tdo {\n\t\t\t\t\t\t--size;\n\t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\t\treturn split;\n\t\t\t\t\t\t}\n\t\t\t\t\t} while (size \u003e 0);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t// we did not find a local split, return any\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\titer.remove();\n\t\t\t\t\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}",
      "path": "flink-core/src/main/java/org/apache/flink/api/common/io/LocatableInputSplitAssigner.java",
      "functionStartLine": 67,
      "functionName": "getNextInputSplit",
      "diff": "@@ -1,109 +1,110 @@\n \tpublic LocatableInputSplit getNextInputSplit(String host) {\n \t\t// for a null host, we return an arbitrary split\n \t\tif (host \u003d\u003d null) {\n \t\t\t\n \t\t\tsynchronized (this.unassigned) {\n \t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n \t\t\t\tif (iter.hasNext()) {\n \t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n \t\t\t\t\titer.remove();\n \t\t\t\t\t\n \t\t\t\t\tif (LOG.isDebugEnabled()) {\n \t\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n \t\t\t\t\t}\n \t\t\t\t\t\n \t\t\t\t\tremoteAssignments++;\n \t\t\t\t\treturn next;\n \t\t\t\t} else {\n \t\t\t\t\tif (LOG.isDebugEnabled()) {\n-\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n+\t\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n \t\t\t\t\t}\n \t\t\t\t\treturn null;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \t\t\n \t\thost \u003d host.toLowerCase(Locale.US);\n \t\t\n \t\t// for any non-null host, we take the list of non-null splits\n \t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n \t\t\n \t\t// if we have no list for this host yet, create one\n \t\tif (localSplits \u003d\u003d null) {\n \t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n \t\t\t\n \t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n \t\t\tsynchronized (localSplits) {\n \t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n \t\t\t\t\n \t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n \t\t\t\t// simply work with that other list\n \t\t\t\tif (prior \u003d\u003d null) {\n \t\t\t\t\t// we are the first, we populate\n \t\t\t\t\t\n \t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n \t\t\t\t\t// because that is shared among threads\n \t\t\t\t\tLocatableInputSplit[] remaining;\n \t\t\t\t\tsynchronized (this.unassigned) {\n \t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n \t\t\t\t\t}\n \t\t\t\t\t\n \t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n \t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n \t\t\t\t\t\t\tlocalSplits.add(is);\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\telse {\n \t\t\t\t\t// someone else was faster\n \t\t\t\t\tlocalSplits \u003d prior;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \t\t\n+\t\t\n \t\t// at this point, we have a list of local splits (possibly empty)\n \t\t// we need to make sure no one else operates in the current list (that protects against\n \t\t// list creation races) and that the unassigned set is consistent\n \t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n \t\tsynchronized (localSplits) {\n \t\t\tint size \u003d localSplits.size();\n \t\t\tif (size \u003e 0) {\n \t\t\t\tsynchronized (this.unassigned) {\n \t\t\t\t\tdo {\n \t\t\t\t\t\t--size;\n \t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n \t\t\t\t\t\tif (this.unassigned.remove(split)) {\n \t\t\t\t\t\t\t\n \t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n \t\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n \t\t\t\t\t\t\t}\n \t\t\t\t\t\t\t\n \t\t\t\t\t\t\tlocalAssignments++;\n \t\t\t\t\t\t\treturn split;\n \t\t\t\t\t\t}\n \t\t\t\t\t} while (size \u003e 0);\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \t\t\n \t\t// we did not find a local split, return any\n \t\tsynchronized (this.unassigned) {\n \t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n \t\t\tif (iter.hasNext()) {\n \t\t\t\tLocatableInputSplit next \u003d iter.next();\n \t\t\t\titer.remove();\n \t\t\t\t\n \t\t\t\tif (LOG.isDebugEnabled()) {\n \t\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\tremoteAssignments++;\n \t\t\t\treturn next;\n \t\t\t} else {\n \t\t\t\tif (LOG.isDebugEnabled()) {\n \t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n \t\t\t\t}\n \t\t\t\treturn null;\n \t\t\t}\n \t\t}\n \t}\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "b32e77a2d8be76aeafa28b94fd7cfbb8de80f4cb": {
      "type": "Yfilerename",
      "commitMessage": "Refactor job graph construction to incremental attachment based\n",
      "commitDate": "2014-09-20, 2:02 p.m.",
      "commitName": "b32e77a2d8be76aeafa28b94fd7cfbb8de80f4cb",
      "commitAuthor": "Stephan Ewen",
      "commitDateOld": "2014-09-20, 2:02 p.m.",
      "commitNameOld": "c16f6d816899d8db7ff7c809d0c26be611b4d561",
      "commitAuthorOld": "Stephan Ewen",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "actualSource": "\tpublic LocatableInputSplit getNextInputSplit(String host) {\n\t\t// for a null host, we return an arbitrary split\n\t\tif (host \u003d\u003d null) {\n\t\t\t\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\t\tif (iter.hasNext()) {\n\t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\t\titer.remove();\n\t\t\t\t\t\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tremoteAssignments++;\n\t\t\t\t\treturn next;\n\t\t\t\t} else {\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\thost \u003d host.toLowerCase(Locale.US);\n\t\t\n\t\t// for any non-null host, we take the list of non-null splits\n\t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n\t\t\n\t\t// if we have no list for this host yet, create one\n\t\tif (localSplits \u003d\u003d null) {\n\t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n\t\t\t\n\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n\t\t\tsynchronized (localSplits) {\n\t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n\t\t\t\t\n\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t\t// simply work with that other list\n\t\t\t\tif (prior \u003d\u003d null) {\n\t\t\t\t\t// we are the first, we populate\n\t\t\t\t\t\n\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t\t// because that is shared among threads\n\t\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// someone else was faster\n\t\t\t\t\tlocalSplits \u003d prior;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t// at this point, we have a list of local splits (possibly empty)\n\t\t// we need to make sure no one else operates in the current list (that protects against\n\t\t// list creation races) and that the unassigned set is consistent\n\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\t\tsynchronized (localSplits) {\n\t\t\tint size \u003d localSplits.size();\n\t\t\tif (size \u003e 0) {\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tdo {\n\t\t\t\t\t\t--size;\n\t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\t\treturn split;\n\t\t\t\t\t\t}\n\t\t\t\t\t} while (size \u003e 0);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t// we did not find a local split, return any\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\titer.remove();\n\t\t\t\t\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}",
      "path": "flink-core/src/main/java/org/apache/flink/api/common/io/LocatableInputSplitAssigner.java",
      "functionStartLine": 66,
      "functionName": "getNextInputSplit",
      "diff": "",
      "extendedDetails": {
        "oldPath": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/splitassigner/LocatableInputSplitAssigner.java",
        "newPath": "flink-core/src/main/java/org/apache/flink/api/common/io/LocatableInputSplitAssigner.java"
      }
    },
    "c32569aed12ffa968e2c2289c2d56db262c0eba4": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "[FLINK-1094] Reworked, improved, and testes split assigners\n",
      "commitDate": "2014-09-20, 2:02 p.m.",
      "commitName": "c32569aed12ffa968e2c2289c2d56db262c0eba4",
      "commitAuthor": "Stephan Ewen",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "[FLINK-1094] Reworked, improved, and testes split assigners\n",
          "commitDate": "2014-09-20, 2:02 p.m.",
          "commitName": "c32569aed12ffa968e2c2289c2d56db262c0eba4",
          "commitAuthor": "Stephan Ewen",
          "commitDateOld": "2014-09-05, 5:57 a.m.",
          "commitNameOld": "08188508d528c1072a746aacbf2a5c712d4f8467",
          "commitAuthorOld": "Till Rohrmann",
          "daysBetweenCommits": 15.34,
          "commitsBetweenForRepo": 34,
          "commitsBetweenForFile": 1,
          "actualSource": "\tpublic LocatableInputSplit getNextInputSplit(String host) {\n\t\t// for a null host, we return an arbitrary split\n\t\tif (host \u003d\u003d null) {\n\t\t\t\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\t\tif (iter.hasNext()) {\n\t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\t\titer.remove();\n\t\t\t\t\t\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tremoteAssignments++;\n\t\t\t\t\treturn next;\n\t\t\t\t} else {\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\thost \u003d host.toLowerCase(Locale.US);\n\t\t\n\t\t// for any non-null host, we take the list of non-null splits\n\t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n\t\t\n\t\t// if we have no list for this host yet, create one\n\t\tif (localSplits \u003d\u003d null) {\n\t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n\t\t\t\n\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n\t\t\tsynchronized (localSplits) {\n\t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n\t\t\t\t\n\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t\t// simply work with that other list\n\t\t\t\tif (prior \u003d\u003d null) {\n\t\t\t\t\t// we are the first, we populate\n\t\t\t\t\t\n\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t\t// because that is shared among threads\n\t\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// someone else was faster\n\t\t\t\t\tlocalSplits \u003d prior;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t// at this point, we have a list of local splits (possibly empty)\n\t\t// we need to make sure no one else operates in the current list (that protects against\n\t\t// list creation races) and that the unassigned set is consistent\n\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\t\tsynchronized (localSplits) {\n\t\t\tint size \u003d localSplits.size();\n\t\t\tif (size \u003e 0) {\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tdo {\n\t\t\t\t\t\t--size;\n\t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\t\treturn split;\n\t\t\t\t\t\t}\n\t\t\t\t\t} while (size \u003e 0);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t// we did not find a local split, return any\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\titer.remove();\n\t\t\t\t\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}",
          "path": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/splitassigner/LocatableInputSplitAssigner.java",
          "functionStartLine": 66,
          "functionName": "getNextInputSplit",
          "diff": "@@ -1,17 +1,109 @@\n-\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n-\n-\t\tfinal ExecutionGroupVertex groupVertex \u003d vertex.getGroupVertex();\n-\t\tfinal LocatableInputSplitList splitStore \u003d this.vertexMap.get(groupVertex);\n-\n-\t\tif (splitStore \u003d\u003d null) {\n-\t\t\treturn null;\n+\tpublic LocatableInputSplit getNextInputSplit(String host) {\n+\t\t// for a null host, we return an arbitrary split\n+\t\tif (host \u003d\u003d null) {\n+\t\t\t\n+\t\t\tsynchronized (this.unassigned) {\n+\t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n+\t\t\t\tif (iter.hasNext()) {\n+\t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n+\t\t\t\t\titer.remove();\n+\t\t\t\t\t\n+\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n+\t\t\t\t\t}\n+\t\t\t\t\t\n+\t\t\t\t\tremoteAssignments++;\n+\t\t\t\t\treturn next;\n+\t\t\t\t} else {\n+\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n+\t\t\t\t\t}\n+\t\t\t\t\treturn null;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n-\n-\t\tfinal Instance instance \u003d vertex.getAllocatedResource().getInstance();\n-\t\tif (instance \u003d\u003d null) {\n-\t\t\tLOG.error(\"Instance is null, returning random split\");\n-\t\t\treturn null;\n+\t\t\n+\t\thost \u003d host.toLowerCase(Locale.US);\n+\t\t\n+\t\t// for any non-null host, we take the list of non-null splits\n+\t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n+\t\t\n+\t\t// if we have no list for this host yet, create one\n+\t\tif (localSplits \u003d\u003d null) {\n+\t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n+\t\t\t\n+\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n+\t\t\tsynchronized (localSplits) {\n+\t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n+\t\t\t\t\n+\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n+\t\t\t\t// simply work with that other list\n+\t\t\t\tif (prior \u003d\u003d null) {\n+\t\t\t\t\t// we are the first, we populate\n+\t\t\t\t\t\n+\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n+\t\t\t\t\t// because that is shared among threads\n+\t\t\t\t\tLocatableInputSplit[] remaining;\n+\t\t\t\t\tsynchronized (this.unassigned) {\n+\t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n+\t\t\t\t\t}\n+\t\t\t\t\t\n+\t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n+\t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n+\t\t\t\t\t\t\tlocalSplits.add(is);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t// someone else was faster\n+\t\t\t\t\tlocalSplits \u003d prior;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n-\n-\t\treturn splitStore.getNextInputSplit(instance);\n+\t\t\n+\t\t// at this point, we have a list of local splits (possibly empty)\n+\t\t// we need to make sure no one else operates in the current list (that protects against\n+\t\t// list creation races) and that the unassigned set is consistent\n+\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n+\t\tsynchronized (localSplits) {\n+\t\t\tint size \u003d localSplits.size();\n+\t\t\tif (size \u003e 0) {\n+\t\t\t\tsynchronized (this.unassigned) {\n+\t\t\t\t\tdo {\n+\t\t\t\t\t\t--size;\n+\t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n+\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n+\t\t\t\t\t\t\t\n+\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\n+\t\t\t\t\t\t\tlocalAssignments++;\n+\t\t\t\t\t\t\treturn split;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} while (size \u003e 0);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\t\n+\t\t// we did not find a local split, return any\n+\t\tsynchronized (this.unassigned) {\n+\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n+\t\t\tif (iter.hasNext()) {\n+\t\t\t\tLocatableInputSplit next \u003d iter.next();\n+\t\t\t\titer.remove();\n+\t\t\t\t\n+\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n+\t\t\t\t}\n+\t\t\t\t\n+\t\t\t\tremoteAssignments++;\n+\t\t\t\treturn next;\n+\t\t\t} else {\n+\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n+\t\t\t\t}\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n \t}\n\\ No newline at end of file\n",
          "extendedDetails": {
            "oldValue": "[vertex-ExecutionVertex(modifiers-final)]",
            "newValue": "[host-String]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "[FLINK-1094] Reworked, improved, and testes split assigners\n",
          "commitDate": "2014-09-20, 2:02 p.m.",
          "commitName": "c32569aed12ffa968e2c2289c2d56db262c0eba4",
          "commitAuthor": "Stephan Ewen",
          "commitDateOld": "2014-09-05, 5:57 a.m.",
          "commitNameOld": "08188508d528c1072a746aacbf2a5c712d4f8467",
          "commitAuthorOld": "Till Rohrmann",
          "daysBetweenCommits": 15.34,
          "commitsBetweenForRepo": 34,
          "commitsBetweenForFile": 1,
          "actualSource": "\tpublic LocatableInputSplit getNextInputSplit(String host) {\n\t\t// for a null host, we return an arbitrary split\n\t\tif (host \u003d\u003d null) {\n\t\t\t\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\t\tif (iter.hasNext()) {\n\t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\t\titer.remove();\n\t\t\t\t\t\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tremoteAssignments++;\n\t\t\t\t\treturn next;\n\t\t\t\t} else {\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\thost \u003d host.toLowerCase(Locale.US);\n\t\t\n\t\t// for any non-null host, we take the list of non-null splits\n\t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n\t\t\n\t\t// if we have no list for this host yet, create one\n\t\tif (localSplits \u003d\u003d null) {\n\t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n\t\t\t\n\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n\t\t\tsynchronized (localSplits) {\n\t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n\t\t\t\t\n\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t\t// simply work with that other list\n\t\t\t\tif (prior \u003d\u003d null) {\n\t\t\t\t\t// we are the first, we populate\n\t\t\t\t\t\n\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t\t// because that is shared among threads\n\t\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// someone else was faster\n\t\t\t\t\tlocalSplits \u003d prior;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t// at this point, we have a list of local splits (possibly empty)\n\t\t// we need to make sure no one else operates in the current list (that protects against\n\t\t// list creation races) and that the unassigned set is consistent\n\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\t\tsynchronized (localSplits) {\n\t\t\tint size \u003d localSplits.size();\n\t\t\tif (size \u003e 0) {\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tdo {\n\t\t\t\t\t\t--size;\n\t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\t\treturn split;\n\t\t\t\t\t\t}\n\t\t\t\t\t} while (size \u003e 0);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t// we did not find a local split, return any\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\titer.remove();\n\t\t\t\t\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}",
          "path": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/splitassigner/LocatableInputSplitAssigner.java",
          "functionStartLine": 66,
          "functionName": "getNextInputSplit",
          "diff": "@@ -1,17 +1,109 @@\n-\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n-\n-\t\tfinal ExecutionGroupVertex groupVertex \u003d vertex.getGroupVertex();\n-\t\tfinal LocatableInputSplitList splitStore \u003d this.vertexMap.get(groupVertex);\n-\n-\t\tif (splitStore \u003d\u003d null) {\n-\t\t\treturn null;\n+\tpublic LocatableInputSplit getNextInputSplit(String host) {\n+\t\t// for a null host, we return an arbitrary split\n+\t\tif (host \u003d\u003d null) {\n+\t\t\t\n+\t\t\tsynchronized (this.unassigned) {\n+\t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n+\t\t\t\tif (iter.hasNext()) {\n+\t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n+\t\t\t\t\titer.remove();\n+\t\t\t\t\t\n+\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n+\t\t\t\t\t}\n+\t\t\t\t\t\n+\t\t\t\t\tremoteAssignments++;\n+\t\t\t\t\treturn next;\n+\t\t\t\t} else {\n+\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n+\t\t\t\t\t}\n+\t\t\t\t\treturn null;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n-\n-\t\tfinal Instance instance \u003d vertex.getAllocatedResource().getInstance();\n-\t\tif (instance \u003d\u003d null) {\n-\t\t\tLOG.error(\"Instance is null, returning random split\");\n-\t\t\treturn null;\n+\t\t\n+\t\thost \u003d host.toLowerCase(Locale.US);\n+\t\t\n+\t\t// for any non-null host, we take the list of non-null splits\n+\t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n+\t\t\n+\t\t// if we have no list for this host yet, create one\n+\t\tif (localSplits \u003d\u003d null) {\n+\t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n+\t\t\t\n+\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n+\t\t\tsynchronized (localSplits) {\n+\t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n+\t\t\t\t\n+\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n+\t\t\t\t// simply work with that other list\n+\t\t\t\tif (prior \u003d\u003d null) {\n+\t\t\t\t\t// we are the first, we populate\n+\t\t\t\t\t\n+\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n+\t\t\t\t\t// because that is shared among threads\n+\t\t\t\t\tLocatableInputSplit[] remaining;\n+\t\t\t\t\tsynchronized (this.unassigned) {\n+\t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n+\t\t\t\t\t}\n+\t\t\t\t\t\n+\t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n+\t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n+\t\t\t\t\t\t\tlocalSplits.add(is);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t// someone else was faster\n+\t\t\t\t\tlocalSplits \u003d prior;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n-\n-\t\treturn splitStore.getNextInputSplit(instance);\n+\t\t\n+\t\t// at this point, we have a list of local splits (possibly empty)\n+\t\t// we need to make sure no one else operates in the current list (that protects against\n+\t\t// list creation races) and that the unassigned set is consistent\n+\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n+\t\tsynchronized (localSplits) {\n+\t\t\tint size \u003d localSplits.size();\n+\t\t\tif (size \u003e 0) {\n+\t\t\t\tsynchronized (this.unassigned) {\n+\t\t\t\t\tdo {\n+\t\t\t\t\t\t--size;\n+\t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n+\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n+\t\t\t\t\t\t\t\n+\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\n+\t\t\t\t\t\t\tlocalAssignments++;\n+\t\t\t\t\t\t\treturn split;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} while (size \u003e 0);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\t\n+\t\t// we did not find a local split, return any\n+\t\tsynchronized (this.unassigned) {\n+\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n+\t\t\tif (iter.hasNext()) {\n+\t\t\t\tLocatableInputSplit next \u003d iter.next();\n+\t\t\t\titer.remove();\n+\t\t\t\t\n+\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n+\t\t\t\t}\n+\t\t\t\t\n+\t\t\t\tremoteAssignments++;\n+\t\t\t\treturn next;\n+\t\t\t} else {\n+\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n+\t\t\t\t}\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n \t}\n\\ No newline at end of file\n",
          "extendedDetails": {
            "oldValue": "InputSplit",
            "newValue": "LocatableInputSplit"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "[FLINK-1094] Reworked, improved, and testes split assigners\n",
          "commitDate": "2014-09-20, 2:02 p.m.",
          "commitName": "c32569aed12ffa968e2c2289c2d56db262c0eba4",
          "commitAuthor": "Stephan Ewen",
          "commitDateOld": "2014-09-05, 5:57 a.m.",
          "commitNameOld": "08188508d528c1072a746aacbf2a5c712d4f8467",
          "commitAuthorOld": "Till Rohrmann",
          "daysBetweenCommits": 15.34,
          "commitsBetweenForRepo": 34,
          "commitsBetweenForFile": 1,
          "actualSource": "\tpublic LocatableInputSplit getNextInputSplit(String host) {\n\t\t// for a null host, we return an arbitrary split\n\t\tif (host \u003d\u003d null) {\n\t\t\t\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\t\tif (iter.hasNext()) {\n\t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\t\titer.remove();\n\t\t\t\t\t\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tremoteAssignments++;\n\t\t\t\t\treturn next;\n\t\t\t\t} else {\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\thost \u003d host.toLowerCase(Locale.US);\n\t\t\n\t\t// for any non-null host, we take the list of non-null splits\n\t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n\t\t\n\t\t// if we have no list for this host yet, create one\n\t\tif (localSplits \u003d\u003d null) {\n\t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n\t\t\t\n\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n\t\t\tsynchronized (localSplits) {\n\t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n\t\t\t\t\n\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t\t// simply work with that other list\n\t\t\t\tif (prior \u003d\u003d null) {\n\t\t\t\t\t// we are the first, we populate\n\t\t\t\t\t\n\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t\t// because that is shared among threads\n\t\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// someone else was faster\n\t\t\t\t\tlocalSplits \u003d prior;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t// at this point, we have a list of local splits (possibly empty)\n\t\t// we need to make sure no one else operates in the current list (that protects against\n\t\t// list creation races) and that the unassigned set is consistent\n\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\t\tsynchronized (localSplits) {\n\t\t\tint size \u003d localSplits.size();\n\t\t\tif (size \u003e 0) {\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tdo {\n\t\t\t\t\t\t--size;\n\t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\t\treturn split;\n\t\t\t\t\t\t}\n\t\t\t\t\t} while (size \u003e 0);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\t// we did not find a local split, return any\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next \u003d iter.next();\n\t\t\t\titer.remove();\n\t\t\t\t\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}",
          "path": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/splitassigner/LocatableInputSplitAssigner.java",
          "functionStartLine": 66,
          "functionName": "getNextInputSplit",
          "diff": "@@ -1,17 +1,109 @@\n-\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n-\n-\t\tfinal ExecutionGroupVertex groupVertex \u003d vertex.getGroupVertex();\n-\t\tfinal LocatableInputSplitList splitStore \u003d this.vertexMap.get(groupVertex);\n-\n-\t\tif (splitStore \u003d\u003d null) {\n-\t\t\treturn null;\n+\tpublic LocatableInputSplit getNextInputSplit(String host) {\n+\t\t// for a null host, we return an arbitrary split\n+\t\tif (host \u003d\u003d null) {\n+\t\t\t\n+\t\t\tsynchronized (this.unassigned) {\n+\t\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n+\t\t\t\tif (iter.hasNext()) {\n+\t\t\t\t\tLocatableInputSplit next \u003d iter.next();\n+\t\t\t\t\titer.remove();\n+\t\t\t\t\t\n+\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n+\t\t\t\t\t}\n+\t\t\t\t\t\n+\t\t\t\t\tremoteAssignments++;\n+\t\t\t\t\treturn next;\n+\t\t\t\t} else {\n+\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n+\t\t\t\t\t}\n+\t\t\t\t\treturn null;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n-\n-\t\tfinal Instance instance \u003d vertex.getAllocatedResource().getInstance();\n-\t\tif (instance \u003d\u003d null) {\n-\t\t\tLOG.error(\"Instance is null, returning random split\");\n-\t\t\treturn null;\n+\t\t\n+\t\thost \u003d host.toLowerCase(Locale.US);\n+\t\t\n+\t\t// for any non-null host, we take the list of non-null splits\n+\t\tList\u003cLocatableInputSplit\u003e localSplits \u003d this.localPerHost.get(host);\n+\t\t\n+\t\t// if we have no list for this host yet, create one\n+\t\tif (localSplits \u003d\u003d null) {\n+\t\t\tlocalSplits \u003d new ArrayList\u003cLocatableInputSplit\u003e(16);\n+\t\t\t\n+\t\t\t// lock the list, to be sure that others have to wait for that host\u0027s local list\n+\t\t\tsynchronized (localSplits) {\n+\t\t\t\tList\u003cLocatableInputSplit\u003e prior \u003d this.localPerHost.putIfAbsent(host, localSplits);\n+\t\t\t\t\n+\t\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n+\t\t\t\t// simply work with that other list\n+\t\t\t\tif (prior \u003d\u003d null) {\n+\t\t\t\t\t// we are the first, we populate\n+\t\t\t\t\t\n+\t\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n+\t\t\t\t\t// because that is shared among threads\n+\t\t\t\t\tLocatableInputSplit[] remaining;\n+\t\t\t\t\tsynchronized (this.unassigned) {\n+\t\t\t\t\t\tremaining \u003d (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n+\t\t\t\t\t}\n+\t\t\t\t\t\n+\t\t\t\t\tfor (LocatableInputSplit is : remaining) {\n+\t\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n+\t\t\t\t\t\t\tlocalSplits.add(is);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t// someone else was faster\n+\t\t\t\t\tlocalSplits \u003d prior;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n-\n-\t\treturn splitStore.getNextInputSplit(instance);\n+\t\t\n+\t\t// at this point, we have a list of local splits (possibly empty)\n+\t\t// we need to make sure no one else operates in the current list (that protects against\n+\t\t// list creation races) and that the unassigned set is consistent\n+\t\t// NOTE: we need to obtain the locks in this order, strictly!!!\n+\t\tsynchronized (localSplits) {\n+\t\t\tint size \u003d localSplits.size();\n+\t\t\tif (size \u003e 0) {\n+\t\t\t\tsynchronized (this.unassigned) {\n+\t\t\t\t\tdo {\n+\t\t\t\t\t\t--size;\n+\t\t\t\t\t\tLocatableInputSplit split \u003d localSplits.remove(size);\n+\t\t\t\t\t\tif (this.unassigned.remove(split)) {\n+\t\t\t\t\t\t\t\n+\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\n+\t\t\t\t\t\t\tlocalAssignments++;\n+\t\t\t\t\t\t\treturn split;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} while (size \u003e 0);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\t\n+\t\t// we did not find a local split, return any\n+\t\tsynchronized (this.unassigned) {\n+\t\t\tIterator\u003cLocatableInputSplit\u003e iter \u003d this.unassigned.iterator();\n+\t\t\tif (iter.hasNext()) {\n+\t\t\t\tLocatableInputSplit next \u003d iter.next();\n+\t\t\t\titer.remove();\n+\t\t\t\t\n+\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n+\t\t\t\t}\n+\t\t\t\t\n+\t\t\t\tremoteAssignments++;\n+\t\t\t\treturn next;\n+\t\t\t} else {\n+\t\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n+\t\t\t\t}\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n \t}\n\\ No newline at end of file\n",
          "extendedDetails": {}
        }
      ]
    },
    "8563d511da8ab8ac0e1362775f11aef7b67375be": {
      "type": "Yfilerename",
      "commitMessage": "Rename POMs, scripts, quickstarts and other minor renames\n",
      "commitDate": "2014-07-10, 4:35 p.m.",
      "commitName": "8563d511da8ab8ac0e1362775f11aef7b67375be",
      "commitAuthor": "Robert Metzger",
      "commitDateOld": "2014-07-10, 10:46 a.m.",
      "commitNameOld": "5f011547378ad03c033770503c6dcfd6b64e9647",
      "commitAuthorOld": "Stephan Ewen",
      "daysBetweenCommits": 0.24,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "actualSource": "\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n\n\t\tfinal ExecutionGroupVertex groupVertex \u003d vertex.getGroupVertex();\n\t\tfinal LocatableInputSplitList splitStore \u003d this.vertexMap.get(groupVertex);\n\n\t\tif (splitStore \u003d\u003d null) {\n\t\t\treturn null;\n\t\t}\n\n\t\tfinal Instance instance \u003d vertex.getAllocatedResource().getInstance();\n\t\tif (instance \u003d\u003d null) {\n\t\t\tLOG.error(\"Instance is null, returning random split\");\n\t\t\treturn null;\n\t\t}\n\n\t\treturn splitStore.getNextInputSplit(instance);\n\t}",
      "path": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/splitassigner/LocatableInputSplitAssigner.java",
      "functionStartLine": 96,
      "functionName": "getNextInputSplit",
      "diff": "",
      "extendedDetails": {
        "oldPath": "stratosphere-runtime/src/main/java/org/apache/flink/runtime/jobmanager/splitassigner/LocatableInputSplitAssigner.java",
        "newPath": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/splitassigner/LocatableInputSplitAssigner.java"
      }
    },
    "e73296f3e3fad9bb715edd0ff7a0eb10ce1226b9": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "Renaming part 3 (runtime)\n",
      "commitDate": "2014-07-09, 2:25 p.m.",
      "commitName": "e73296f3e3fad9bb715edd0ff7a0eb10ce1226b9",
      "commitAuthor": "Stephan Ewen",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "Renaming part 3 (runtime)\n",
          "commitDate": "2014-07-09, 2:25 p.m.",
          "commitName": "e73296f3e3fad9bb715edd0ff7a0eb10ce1226b9",
          "commitAuthor": "Stephan Ewen",
          "commitDateOld": "2014-07-09, 1:14 p.m.",
          "commitNameOld": "24eb47ac86299920aa137d5b6394a248ff58d19e",
          "commitAuthorOld": "Stephan Ewen",
          "daysBetweenCommits": 0.05,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "actualSource": "\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n\n\t\tfinal ExecutionGroupVertex groupVertex \u003d vertex.getGroupVertex();\n\t\tfinal LocatableInputSplitList splitStore \u003d this.vertexMap.get(groupVertex);\n\n\t\tif (splitStore \u003d\u003d null) {\n\t\t\treturn null;\n\t\t}\n\n\t\tfinal Instance instance \u003d vertex.getAllocatedResource().getInstance();\n\t\tif (instance \u003d\u003d null) {\n\t\t\tLOG.error(\"Instance is null, returning random split\");\n\t\t\treturn null;\n\t\t}\n\n\t\treturn splitStore.getNextInputSplit(instance);\n\t}",
          "path": "stratosphere-runtime/src/main/java/org/apache/flink/runtime/jobmanager/splitassigner/LocatableInputSplitAssigner.java",
          "functionStartLine": 96,
          "functionName": "getNextInputSplit",
          "diff": "@@ -1,17 +1,17 @@\n \tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n \n-\t\tfinal Queue\u003cInputSplit\u003e queue \u003d this.splitMap.get(vertex.getGroupVertex());\n-\t\tif (queue \u003d\u003d null) {\n-\t\t\tfinal JobID jobID \u003d vertex.getExecutionGraph().getJobID();\n-\t\t\tLOG.error(\"Cannot find split queue for vertex \" + vertex.getGroupVertex() + \" (job \" + jobID + \")\");\n+\t\tfinal ExecutionGroupVertex groupVertex \u003d vertex.getGroupVertex();\n+\t\tfinal LocatableInputSplitList splitStore \u003d this.vertexMap.get(groupVertex);\n+\n+\t\tif (splitStore \u003d\u003d null) {\n \t\t\treturn null;\n \t\t}\n \n-\t\tInputSplit nextSplit \u003d queue.poll();\n-\n-\t\tif (LOG.isDebugEnabled() \u0026\u0026 nextSplit !\u003d null) {\n-\t\t\tLOG.debug(\"Assigning split \" + nextSplit.getSplitNumber() + \" to \" + vertex);\n+\t\tfinal Instance instance \u003d vertex.getAllocatedResource().getInstance();\n+\t\tif (instance \u003d\u003d null) {\n+\t\t\tLOG.error(\"Instance is null, returning random split\");\n+\t\t\treturn null;\n \t\t}\n \n-\t\treturn nextSplit;\n+\t\treturn splitStore.getNextInputSplit(instance);\n \t}\n\\ No newline at end of file\n",
          "extendedDetails": {
            "oldPath": "stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/splitassigner/DefaultInputSplitAssigner.java",
            "newPath": "stratosphere-runtime/src/main/java/org/apache/flink/runtime/jobmanager/splitassigner/LocatableInputSplitAssigner.java",
            "oldMethodName": "getNextInputSplit",
            "newMethodName": "getNextInputSplit"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Renaming part 3 (runtime)\n",
          "commitDate": "2014-07-09, 2:25 p.m.",
          "commitName": "e73296f3e3fad9bb715edd0ff7a0eb10ce1226b9",
          "commitAuthor": "Stephan Ewen",
          "commitDateOld": "2014-07-09, 1:14 p.m.",
          "commitNameOld": "24eb47ac86299920aa137d5b6394a248ff58d19e",
          "commitAuthorOld": "Stephan Ewen",
          "daysBetweenCommits": 0.05,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "actualSource": "\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n\n\t\tfinal ExecutionGroupVertex groupVertex \u003d vertex.getGroupVertex();\n\t\tfinal LocatableInputSplitList splitStore \u003d this.vertexMap.get(groupVertex);\n\n\t\tif (splitStore \u003d\u003d null) {\n\t\t\treturn null;\n\t\t}\n\n\t\tfinal Instance instance \u003d vertex.getAllocatedResource().getInstance();\n\t\tif (instance \u003d\u003d null) {\n\t\t\tLOG.error(\"Instance is null, returning random split\");\n\t\t\treturn null;\n\t\t}\n\n\t\treturn splitStore.getNextInputSplit(instance);\n\t}",
          "path": "stratosphere-runtime/src/main/java/org/apache/flink/runtime/jobmanager/splitassigner/LocatableInputSplitAssigner.java",
          "functionStartLine": 96,
          "functionName": "getNextInputSplit",
          "diff": "@@ -1,17 +1,17 @@\n \tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n \n-\t\tfinal Queue\u003cInputSplit\u003e queue \u003d this.splitMap.get(vertex.getGroupVertex());\n-\t\tif (queue \u003d\u003d null) {\n-\t\t\tfinal JobID jobID \u003d vertex.getExecutionGraph().getJobID();\n-\t\t\tLOG.error(\"Cannot find split queue for vertex \" + vertex.getGroupVertex() + \" (job \" + jobID + \")\");\n+\t\tfinal ExecutionGroupVertex groupVertex \u003d vertex.getGroupVertex();\n+\t\tfinal LocatableInputSplitList splitStore \u003d this.vertexMap.get(groupVertex);\n+\n+\t\tif (splitStore \u003d\u003d null) {\n \t\t\treturn null;\n \t\t}\n \n-\t\tInputSplit nextSplit \u003d queue.poll();\n-\n-\t\tif (LOG.isDebugEnabled() \u0026\u0026 nextSplit !\u003d null) {\n-\t\t\tLOG.debug(\"Assigning split \" + nextSplit.getSplitNumber() + \" to \" + vertex);\n+\t\tfinal Instance instance \u003d vertex.getAllocatedResource().getInstance();\n+\t\tif (instance \u003d\u003d null) {\n+\t\t\tLOG.error(\"Instance is null, returning random split\");\n+\t\t\treturn null;\n \t\t}\n \n-\t\treturn nextSplit;\n+\t\treturn splitStore.getNextInputSplit(instance);\n \t}\n\\ No newline at end of file\n",
          "extendedDetails": {}
        }
      ]
    },
    "33cb2ca9898809d2fc90765996ea56bbea458e59": {
      "type": "Yfilerename",
      "commitMessage": "Reorganized basic project structures. No renamings yet.\n",
      "commitDate": "2013-12-13, 2:52 p.m.",
      "commitName": "33cb2ca9898809d2fc90765996ea56bbea458e59",
      "commitAuthor": "StephanEwen",
      "commitDateOld": "2013-12-13, 12:11 p.m.",
      "commitNameOld": "0ad1dcc7dbd432d01d3cb4ed2db86b79534c68ad",
      "commitAuthorOld": "StephanEwen",
      "daysBetweenCommits": 0.11,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "actualSource": "\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n\n\t\tfinal Queue\u003cInputSplit\u003e queue \u003d this.splitMap.get(vertex.getGroupVertex());\n\t\tif (queue \u003d\u003d null) {\n\t\t\tfinal JobID jobID \u003d vertex.getExecutionGraph().getJobID();\n\t\t\tLOG.error(\"Cannot find split queue for vertex \" + vertex.getGroupVertex() + \" (job \" + jobID + \")\");\n\t\t\treturn null;\n\t\t}\n\n\t\tInputSplit nextSplit \u003d queue.poll();\n\n\t\tif (LOG.isDebugEnabled() \u0026\u0026 nextSplit !\u003d null) {\n\t\t\tLOG.debug(\"Assigning split \" + nextSplit.getSplitNumber() + \" to \" + vertex);\n\t\t}\n\n\t\treturn nextSplit;\n\t}",
      "path": "stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/splitassigner/DefaultInputSplitAssigner.java",
      "functionStartLine": 90,
      "functionName": "getNextInputSplit",
      "diff": "",
      "extendedDetails": {
        "oldPath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/jobmanager/splitassigner/DefaultInputSplitAssigner.java",
        "newPath": "stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/splitassigner/DefaultInputSplitAssigner.java"
      }
    },
    "b407f8b37a4ded820a25cc1cc03da064988c2f0d": {
      "type": "Ybodychange",
      "commitMessage": "Cloudera distribution of hadoop added as default dependency.\nSeparated classpaths for cluster and clients.\n",
      "commitDate": "2013-07-10, 11:36 p.m.",
      "commitName": "b407f8b37a4ded820a25cc1cc03da064988c2f0d",
      "commitAuthor": "Marcus Leich",
      "commitDateOld": "2011-09-05, 12:13 p.m.",
      "commitNameOld": "6651d55a106c85be53373dd3877900f1ffc80793",
      "commitAuthorOld": "Matthias Ringwald",
      "daysBetweenCommits": 674.47,
      "commitsBetweenForRepo": 1894,
      "commitsBetweenForFile": 1,
      "actualSource": "\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n\n\t\tfinal Queue\u003cInputSplit\u003e queue \u003d this.splitMap.get(vertex.getGroupVertex());\n\t\tif (queue \u003d\u003d null) {\n\t\t\tfinal JobID jobID \u003d vertex.getExecutionGraph().getJobID();\n\t\t\tLOG.error(\"Cannot find split queue for vertex \" + vertex.getGroupVertex() + \" (job \" + jobID + \")\");\n\t\t\treturn null;\n\t\t}\n\n\t\tInputSplit nextSplit \u003d queue.poll();\n\n\t\tif (LOG.isDebugEnabled() \u0026\u0026 nextSplit !\u003d null) {\n\t\t\tLOG.debug(\"Assigning split \" + nextSplit.getSplitNumber() + \" to \" + vertex);\n\t\t}\n\n\t\treturn nextSplit;\n\t}",
      "path": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/jobmanager/splitassigner/DefaultInputSplitAssigner.java",
      "functionStartLine": 90,
      "functionName": "getNextInputSplit",
      "diff": "@@ -1,17 +1,17 @@\n \tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n \n \t\tfinal Queue\u003cInputSplit\u003e queue \u003d this.splitMap.get(vertex.getGroupVertex());\n \t\tif (queue \u003d\u003d null) {\n \t\t\tfinal JobID jobID \u003d vertex.getExecutionGraph().getJobID();\n \t\t\tLOG.error(\"Cannot find split queue for vertex \" + vertex.getGroupVertex() + \" (job \" + jobID + \")\");\n \t\t\treturn null;\n \t\t}\n \n \t\tInputSplit nextSplit \u003d queue.poll();\n \n-\t\tif (LOG.isDebugEnabled()) {\n+\t\tif (LOG.isDebugEnabled() \u0026\u0026 nextSplit !\u003d null) {\n \t\t\tLOG.debug(\"Assigning split \" + nextSplit.getSplitNumber() + \" to \" + vertex);\n \t\t}\n \n \t\treturn nextSplit;\n \t}\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "6651d55a106c85be53373dd3877900f1ffc80793": {
      "type": "Ybodychange",
      "commitMessage": "Merged new datamodel with version02",
      "commitDate": "2011-09-05, 12:13 p.m.",
      "commitName": "6651d55a106c85be53373dd3877900f1ffc80793",
      "commitAuthor": "Matthias Ringwald",
      "commitDateOld": "2011-06-15, 12:39 p.m.",
      "commitNameOld": "1e17928631239d0f546e37069929170cab8363ff",
      "commitAuthorOld": "Daniel Warneke",
      "daysBetweenCommits": 81.98,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "actualSource": "\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n\n\t\tfinal Queue\u003cInputSplit\u003e queue \u003d this.splitMap.get(vertex.getGroupVertex());\n\t\tif (queue \u003d\u003d null) {\n\t\t\tfinal JobID jobID \u003d vertex.getExecutionGraph().getJobID();\n\t\t\tLOG.error(\"Cannot find split queue for vertex \" + vertex.getGroupVertex() + \" (job \" + jobID + \")\");\n\t\t\treturn null;\n\t\t}\n\n\t\tInputSplit nextSplit \u003d queue.poll();\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"Assigning split \" + nextSplit.getSplitNumber() + \" to \" + vertex);\n\t\t}\n\n\t\treturn nextSplit;\n\t}",
      "path": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/jobmanager/splitassigner/DefaultInputSplitAssigner.java",
      "functionStartLine": 90,
      "functionName": "getNextInputSplit",
      "diff": "@@ -1,17 +1,17 @@\n \tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n \n \t\tfinal Queue\u003cInputSplit\u003e queue \u003d this.splitMap.get(vertex.getGroupVertex());\n \t\tif (queue \u003d\u003d null) {\n \t\t\tfinal JobID jobID \u003d vertex.getExecutionGraph().getJobID();\n \t\t\tLOG.error(\"Cannot find split queue for vertex \" + vertex.getGroupVertex() + \" (job \" + jobID + \")\");\n \t\t\treturn null;\n \t\t}\n \n \t\tInputSplit nextSplit \u003d queue.poll();\n \n \t\tif (LOG.isDebugEnabled()) {\n-\t\t\tLOG.debug(\"Assigning split \" + nextSplit.getPartitionNumber() + \" to \" + vertex);\n+\t\t\tLOG.debug(\"Assigning split \" + nextSplit.getSplitNumber() + \" to \" + vertex);\n \t\t}\n \n \t\treturn nextSplit;\n \t}\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "1e17928631239d0f546e37069929170cab8363ff": {
      "type": "Yintroduced",
      "commitMessage": "Replaced dummy split assignment code with final input split manager\n",
      "commitDate": "2011-06-15, 12:39 p.m.",
      "commitName": "1e17928631239d0f546e37069929170cab8363ff",
      "commitAuthor": "Daniel Warneke",
      "diff": "@@ -0,0 +1,17 @@\n+\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n+\n+\t\tfinal Queue\u003cInputSplit\u003e queue \u003d this.splitMap.get(vertex.getGroupVertex());\n+\t\tif (queue \u003d\u003d null) {\n+\t\t\tfinal JobID jobID \u003d vertex.getExecutionGraph().getJobID();\n+\t\t\tLOG.error(\"Cannot find split queue for vertex \" + vertex.getGroupVertex() + \" (job \" + jobID + \")\");\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tInputSplit nextSplit \u003d queue.poll();\n+\n+\t\tif (LOG.isDebugEnabled()) {\n+\t\t\tLOG.debug(\"Assigning split \" + nextSplit.getPartitionNumber() + \" to \" + vertex);\n+\t\t}\n+\n+\t\treturn nextSplit;\n+\t}\n\\ No newline at end of file\n",
      "actualSource": "\tpublic InputSplit getNextInputSplit(final ExecutionVertex vertex) {\n\n\t\tfinal Queue\u003cInputSplit\u003e queue \u003d this.splitMap.get(vertex.getGroupVertex());\n\t\tif (queue \u003d\u003d null) {\n\t\t\tfinal JobID jobID \u003d vertex.getExecutionGraph().getJobID();\n\t\t\tLOG.error(\"Cannot find split queue for vertex \" + vertex.getGroupVertex() + \" (job \" + jobID + \")\");\n\t\t\treturn null;\n\t\t}\n\n\t\tInputSplit nextSplit \u003d queue.poll();\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"Assigning split \" + nextSplit.getPartitionNumber() + \" to \" + vertex);\n\t\t}\n\n\t\treturn nextSplit;\n\t}",
      "path": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/jobmanager/splitassigner/DefaultInputSplitAssigner.java",
      "functionStartLine": 90,
      "functionName": "getNextInputSplit"
    }
  }
}